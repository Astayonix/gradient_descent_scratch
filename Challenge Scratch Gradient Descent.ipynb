{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy is the only dependency needed for this demo primarily to read in the data and to create the arrays necessary to calculate the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The slope-line intercept formula is $y=mx+b$ where $m$ is the slope and $b$ is the y-intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, we programmatically compute the following error formula for the sum of squared errors:\n",
    "### $$Error_{(m, b)} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i-(mx_i + b))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_for_line_given_points(b, m, data_points):\n",
    "    #Finds the error between the actual value of y and the predicted value of y (the sum of squared distances).\n",
    "    #Initialize error to 0\n",
    "    total_error = 0\n",
    "    #for every point in our data_points\n",
    "    for i in range(len(data_points)):\n",
    "        #get the x value\n",
    "        x = data_points[i, 0]\n",
    "        #get the y value\n",
    "        y = data_points[i, 1]\n",
    "        #get the difference between actual and predicited values of y, square them (so they're positive), and then add it to the\n",
    "        #total error\n",
    "        total_error += (y - (m * x + b)) **2\n",
    "    #return the average of the difference error squared\n",
    "    return total_error / float(len(data_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, we programmatically compute the partial derivatives of both $b$ and $m$ respectively.  The tagent line has a slope of:\n",
    "### $$ \\frac {\\partial f}{\\partial x}(a, b)$$\n",
    "\n",
    "### Specifically for $m$:\n",
    "### $$ \\frac {\\partial}{\\partial m} = \\frac{2}{N} \\sum_{i=1}^{N} - x_i(y_i-(mx_i + b))$$\n",
    "\n",
    "### Specifically for $b$:\n",
    "### $$ \\frac {\\partial}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^{N} -(y_i-(mx_i + b))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(current_b, current_m, data_points, learning_rate):\n",
    "    #calculates the partial derivitive to achieve the local optima/ convex minima.\n",
    "    #initialize the b and m gradients and N\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(data_points))\n",
    "    \n",
    "    #the error will be a compass to tell our gradient/ slope which direction it should be going.  When the error is smallest,\n",
    "    #that's when we have the line of best fit.\n",
    "    for i in range(len(data_points)):\n",
    "        #getting the x-value\n",
    "        x = data_points[i, 0]\n",
    "        #getting the y-value\n",
    "        y = data_points[i, 1]\n",
    "        #computing the partial derivative of our error function for b\n",
    "        b_gradient += -(2/N) * (y - ((current_m * x) + current_b))\n",
    "        #computing the partial derivative of our error function for m\n",
    "        m_gradient += -(2/N) * x * (y - ((current_m * x) + current_b))\n",
    "    \n",
    "    #using our partial derivatives, we will update the b and m values respectively\n",
    "    new_b = current_b - (learning_rate * b_gradient)\n",
    "    new_m = current_m - (learning_rate * m_gradient)\n",
    "    return [new_b, new_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_run(data_points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    #Intializing b and m\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    \n",
    "    #Gradient descent\n",
    "    for i in range(num_iterations):\n",
    "        #update b and m with the new, more accurate b and m by performing this gradient step\n",
    "        b, m = step_gradient(b, m, np.array(data_points), learning_rate)\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    #Step 1 - Collect the data\n",
    "    data_points = np.genfromtxt('challenge_dataset.txt', delimiter=',')\n",
    "    #data_points = np.genfromtxt('linear_regression_live/data.csv', delimiter=',')\n",
    "    print data_points\n",
    "    \n",
    "    #Step 2 - Build the model/ Define hyperparameters\n",
    "    #How fast should the model reach convergence?\n",
    "    learning_rate = 0.0001\n",
    "    #y = mx+b is slope line intercept formula where m is the slope of the line and b is the y-intercept\n",
    "    initial_b = 0 #initial guess of y-intercept at 0\n",
    "    initial_m = 0 #initial guess of a horizontal line\n",
    "    num_iterations = 5000 #train the model 5,000 times\n",
    "    \n",
    "    #Step 3 - Train the model\n",
    "    print 'Starting gradient descent at b = %s, m = %s, and squared-error = %s' % (initial_b, initial_m, error_for_line_given_points(initial_b, initial_m, data_points))\n",
    "    [b, m] = gradient_descent_run(data_points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    \n",
    "    print 'After %s iterations, b = %s, m = %s, and squared-error = %s ' % (num_iterations, b, m, error_for_line_given_points(b, m, data_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.1101   17.592  ]\n",
      " [  5.5277    9.1302 ]\n",
      " [  8.5186   13.662  ]\n",
      " [  7.0032   11.854  ]\n",
      " [  5.8598    6.8233 ]\n",
      " [  8.3829   11.886  ]\n",
      " [  7.4764    4.3483 ]\n",
      " [  8.5781   12.     ]\n",
      " [  6.4862    6.5987 ]\n",
      " [  5.0546    3.8166 ]\n",
      " [  5.7107    3.2522 ]\n",
      " [ 14.164    15.505  ]\n",
      " [  5.734     3.1551 ]\n",
      " [  8.4084    7.2258 ]\n",
      " [  5.6407    0.71618]\n",
      " [  5.3794    3.5129 ]\n",
      " [  6.3654    5.3048 ]\n",
      " [  5.1301    0.56077]\n",
      " [  6.4296    3.6518 ]\n",
      " [  7.0708    5.3893 ]\n",
      " [  6.1891    3.1386 ]\n",
      " [ 20.27     21.767  ]\n",
      " [  5.4901    4.263  ]\n",
      " [  6.3261    5.1875 ]\n",
      " [  5.5649    3.0825 ]\n",
      " [ 18.945    22.638  ]\n",
      " [ 12.828    13.501  ]\n",
      " [ 10.957     7.0467 ]\n",
      " [ 13.176    14.692  ]\n",
      " [ 22.203    24.147  ]\n",
      " [  5.2524   -1.22   ]\n",
      " [  6.5894    5.9966 ]\n",
      " [  9.2482   12.134  ]\n",
      " [  5.8918    1.8495 ]\n",
      " [  8.2111    6.5426 ]\n",
      " [  7.9334    4.5623 ]\n",
      " [  8.0959    4.1164 ]\n",
      " [  5.6063    3.3928 ]\n",
      " [ 12.836    10.117  ]\n",
      " [  6.3534    5.4974 ]\n",
      " [  5.4069    0.55657]\n",
      " [  6.8825    3.9115 ]\n",
      " [ 11.708     5.3854 ]\n",
      " [  5.7737    2.4406 ]\n",
      " [  7.8247    6.7318 ]\n",
      " [  7.0931    1.0463 ]\n",
      " [  5.0702    5.1337 ]\n",
      " [  5.8014    1.844  ]\n",
      " [ 11.7       8.0043 ]\n",
      " [  5.5416    1.0179 ]\n",
      " [  7.5402    6.7504 ]\n",
      " [  5.3077    1.8396 ]\n",
      " [  7.4239    4.2885 ]\n",
      " [  7.6031    4.9981 ]\n",
      " [  6.3328    1.4233 ]\n",
      " [  6.3589   -1.4211 ]\n",
      " [  6.2742    2.4756 ]\n",
      " [  5.6397    4.6042 ]\n",
      " [  9.3102    3.9624 ]\n",
      " [  9.4536    5.4141 ]\n",
      " [  8.8254    5.1694 ]\n",
      " [  5.1793   -0.74279]\n",
      " [ 21.279    17.929  ]\n",
      " [ 14.908    12.054  ]\n",
      " [ 18.959    17.054  ]\n",
      " [  7.2182    4.8852 ]\n",
      " [  8.2951    5.7442 ]\n",
      " [ 10.236     7.7754 ]\n",
      " [  5.4994    1.0173 ]\n",
      " [ 20.341    20.992  ]\n",
      " [ 10.136     6.6799 ]\n",
      " [  7.3345    4.0259 ]\n",
      " [  6.0062    1.2784 ]\n",
      " [  7.2259    3.3411 ]\n",
      " [  5.0269   -2.6807 ]\n",
      " [  6.5479    0.29678]\n",
      " [  7.5386    3.8845 ]\n",
      " [  5.0365    5.7014 ]\n",
      " [ 10.274     6.7526 ]\n",
      " [  5.1077    2.0576 ]\n",
      " [  5.7292    0.47953]\n",
      " [  5.1884    0.20421]\n",
      " [  6.3557    0.67861]\n",
      " [  9.7687    7.5435 ]\n",
      " [  6.5159    5.3436 ]\n",
      " [  8.5172    4.2415 ]\n",
      " [  9.1802    6.7981 ]\n",
      " [  6.002     0.92695]\n",
      " [  5.5204    0.152  ]\n",
      " [  5.0594    2.8214 ]\n",
      " [  5.7077    1.8451 ]\n",
      " [  7.6366    4.2959 ]\n",
      " [  5.8707    7.2029 ]\n",
      " [  5.3054    1.9869 ]\n",
      " [  8.2934    0.14454]\n",
      " [ 13.394     9.0551 ]\n",
      " [  5.4369    0.61705]]\n",
      "Starting gradient descent at b = 0, m = 0, and squared-error = 64.1454677549\n",
      "After 5000 iterations, b = -0.5760270584, m = 0.859528364212, and squared-error = 10.9605908333 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
