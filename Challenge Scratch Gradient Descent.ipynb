{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy is the only dependency needed for this demo primarily to read in the data and to create the arrays necessary to calculate the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The slope-line intercept formula is $y=mx+b$ where $m$ is the slope and $b$ is the y-intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, we programmatically compute the following error formula for the sum of squared errors:\n",
    "### $$Error_{(m, b)} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i-(mx_i + b))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_for_line_given_points(m, b, data_points):\n",
    "    #Finds the error between the actual value of y and the predicted value of y (the sum of squared distances).\n",
    "    #Initialize error to 0\n",
    "    total_error = 0\n",
    "    #for every point in our data_points\n",
    "    for i in range(len(data_points)):\n",
    "        #get the x value\n",
    "        x = data_points[i, 0]\n",
    "        #print x, i\n",
    "        #get the y value\n",
    "        y = data_points[i, 1]\n",
    "        #print y, i\n",
    "        #get the difference between actual and predicited values of y, square them (so they're positive), and then add it to the\n",
    "        #total error\n",
    "        total_error += (y - (m * x + b)) **2\n",
    "    #return the average of the difference error squared\n",
    "    return total_error / float(len(data_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, we programmatically compute the partial derivatives of both $b$ and $m$ respectively.  The tagent line has a slope of:\n",
    "### $$ \\frac {\\partial f}{\\partial x}(a, b)$$\n",
    "\n",
    "### Specifically for $m$:\n",
    "### $$ \\frac {\\partial}{\\partial m} = \\frac{2}{N} \\sum_{i=1}^{N} - x_i(y_i-(mx_i + b))$$\n",
    "\n",
    "### Specifically for $b$:\n",
    "### $$ \\frac {\\partial}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^{N} -(y_i-(mx_i + b))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(current_b, current_m, data_points, learning_rate):\n",
    "    #calculates the partial derivitive to achieve the local optima/ convex minima.\n",
    "    #initialize the b and m gradients\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(data_points))\n",
    "    #print N\n",
    "    \n",
    "    #the error will be a compass to tell our gradient/ slope which direction it should be going.  When the error is smallest,\n",
    "    #that's when we have the line of best fit.\n",
    "    for i in range(len(data_points)):\n",
    "        #getting the x-value\n",
    "        x = data_points[i, 0]\n",
    "        #getting the y-value\n",
    "        y = data_points[i, 1]\n",
    "        #computing the partial derivative of our error function for b\n",
    "        b_gradient += (2/N) * -(y - ((current_m * x) + current_b))\n",
    "        #computing the partial derivative of our error function for m\n",
    "        m_gradient += (2/N) * -x * (y - ((current_m * x) + current_b))\n",
    "    \n",
    "    #using our partial derivatives, we will update the b and m values respectively\n",
    "    new_b = current_b - (learning_rate * b_gradient)\n",
    "    new_m = current_m - (learning_rate * m_gradient)\n",
    "    return [new_b, new_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_run(data_points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    #Intializing b and m\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    \n",
    "    #Gradient descent\n",
    "    for i in range(num_iterations):\n",
    "        #update b and m with the new, more accurate b and m by performing this gradient step\n",
    "        b, m = step_gradient(b, m, np.array(data_points), learning_rate)\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    #Step 1 - Collect the data\n",
    "    #data_points = np.genfromtxt('challenge_dataset.txt', delimiter=',')\n",
    "    data_points = np.genfromtxt('linear_regression_live/data.csv', delimiter=',')\n",
    "    print data_points\n",
    "    \n",
    "    #Step 2 - Build the model/ Define hyperparameters\n",
    "    #How fast should the model reach convergence?\n",
    "    learning_rate = 0.0001\n",
    "    #y = mx+b is slope line intercept formula where m is the slope of the line and b is the y-intercept\n",
    "    initial_b = 0 #intercept at 0,0\n",
    "    initial_m = 0 #horizontal line\n",
    "    num_iterations = 5000 #we'll train the model 5000 times\n",
    "    \n",
    "    #Step 3 - Train the model\n",
    "    #print 'Starting gradient descent at m = %d, b = %d, and squared-error = %d' % (initial_m, initial_b, error_for_line_given_points(initial_m, initial_b, data_points))\n",
    "    [b, m] = gradient_descent_run(initial_m, initial_b, data_points, learning_rate, num_iterations)\n",
    "    \n",
    "    #print 'Ending point at m = %d, b = %d, and squared-error = %d after %d iterations' % (m, b, error_for_line_given_points(m,b, data_points), num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  32.50234527   31.70700585]\n",
      " [  53.42680403   68.77759598]\n",
      " [  61.53035803   62.5623823 ]\n",
      " [  47.47563963   71.54663223]\n",
      " [  59.81320787   87.23092513]\n",
      " [  55.14218841   78.21151827]\n",
      " [  52.21179669   79.64197305]\n",
      " [  39.29956669   59.17148932]\n",
      " [  48.10504169   75.3312423 ]\n",
      " [  52.55001444   71.30087989]\n",
      " [  45.41973014   55.16567715]\n",
      " [  54.35163488   82.47884676]\n",
      " [  44.1640495    62.00892325]\n",
      " [  58.16847072   75.39287043]\n",
      " [  56.72720806   81.43619216]\n",
      " [  48.95588857   60.72360244]\n",
      " [  44.68719623   82.89250373]\n",
      " [  60.29732685   97.37989686]\n",
      " [  45.61864377   48.84715332]\n",
      " [  38.81681754   56.87721319]\n",
      " [  66.18981661   83.87856466]\n",
      " [  65.41605175  118.5912173 ]\n",
      " [  47.48120861   57.25181946]\n",
      " [  41.57564262   51.39174408]\n",
      " [  51.84518691   75.38065167]\n",
      " [  59.37082201   74.76556403]\n",
      " [  57.31000344   95.45505292]\n",
      " [  63.61556125   95.22936602]\n",
      " [  46.73761941   79.05240617]\n",
      " [  50.55676015   83.43207142]\n",
      " [  52.22399609   63.35879032]\n",
      " [  35.56783005   41.4128853 ]\n",
      " [  42.43647694   76.61734128]\n",
      " [  58.16454011   96.76956643]\n",
      " [  57.50444762   74.08413012]\n",
      " [  45.44053073   66.58814441]\n",
      " [  61.89622268   77.76848242]\n",
      " [  33.09383174   50.71958891]\n",
      " [  36.43600951   62.12457082]\n",
      " [  37.67565486   60.81024665]\n",
      " [  44.55560838   52.68298337]\n",
      " [  43.31828263   58.56982472]\n",
      " [  50.07314563   82.90598149]\n",
      " [  43.87061265   61.4247098 ]\n",
      " [  62.99748075  115.2441528 ]\n",
      " [  32.66904376   45.57058882]\n",
      " [  40.16689901   54.0840548 ]\n",
      " [  53.57507753   87.99445276]\n",
      " [  33.86421497   52.72549438]\n",
      " [  64.70713867   93.57611869]\n",
      " [  38.11982403   80.16627545]\n",
      " [  44.50253806   65.10171157]\n",
      " [  40.59953838   65.56230126]\n",
      " [  41.72067636   65.28088692]\n",
      " [  51.08863468   73.43464155]\n",
      " [  55.0780959    71.13972786]\n",
      " [  41.37772653   79.10282968]\n",
      " [  62.49469743   86.52053844]\n",
      " [  49.20388754   84.74269781]\n",
      " [  41.10268519   59.35885025]\n",
      " [  41.18201611   61.68403752]\n",
      " [  50.18638949   69.84760416]\n",
      " [  52.37844622   86.09829121]\n",
      " [  50.13548549   59.10883927]\n",
      " [  33.64470601   69.89968164]\n",
      " [  39.55790122   44.86249071]\n",
      " [  56.13038882   85.49806778]\n",
      " [  57.36205213   95.53668685]\n",
      " [  60.26921439   70.25193442]\n",
      " [  35.67809389   52.72173496]\n",
      " [  31.588117     50.39267014]\n",
      " [  53.66093226   63.64239878]\n",
      " [  46.68222865   72.24725107]\n",
      " [  43.10782022   57.81251298]\n",
      " [  70.34607562  104.25710159]\n",
      " [  44.49285588   86.64202032]\n",
      " [  57.5045333    91.486778  ]\n",
      " [  36.93007661   55.23166089]\n",
      " [  55.80573336   79.55043668]\n",
      " [  38.95476907   44.84712424]\n",
      " [  56.9012147    80.20752314]\n",
      " [  56.86890066   83.14274979]\n",
      " [  34.3331247    55.72348926]\n",
      " [  59.04974121   77.63418251]\n",
      " [  57.78822399   99.05141484]\n",
      " [  54.28232871   79.12064627]\n",
      " [  51.0887199    69.58889785]\n",
      " [  50.28283635   69.51050331]\n",
      " [  44.21174175   73.68756432]\n",
      " [  38.00548801   61.36690454]\n",
      " [  32.94047994   67.17065577]\n",
      " [  53.69163957   85.66820315]\n",
      " [  68.76573427  114.85387123]\n",
      " [  46.2309665    90.12357207]\n",
      " [  68.31936082   97.91982104]\n",
      " [  50.03017434   81.53699078]\n",
      " [  49.23976534   72.11183247]\n",
      " [  50.03957594   85.23200734]\n",
      " [  48.14985889   66.22495789]\n",
      " [  25.12848465   53.45439421]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ca58371f37ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-f88d48cd544e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[1;31m#Step 3 - Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[1;31m#print 'Starting gradient descent at m = %d, b = %d, and squared-error = %d' % (initial_m, initial_b, error_for_line_given_points(initial_m, initial_b, data_points))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[1;31m#print 'Ending point at m = %d, b = %d, and squared-error = %d after %d iterations' % (m, b, error_for_line_given_points(m,b, data_points), num_iterations)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c02a2d852a28>\u001b[0m in \u001b[0;36mgradient_descent_run\u001b[0;34m(data_points, starting_b, starting_m, learning_rate, num_iterations)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[1;31m#update b and m with the new, more accurate b and m by performing this gradient step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ac7ad7934d92>\u001b[0m in \u001b[0;36mstep_gradient\u001b[0;34m(current_b, current_m, data_points, learning_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mb_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mm_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[1;31m#print N\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
